{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: down\n",
      "reward: 0.0\n",
      "psi: [-1 -1 -1]\n",
      "vector: [  2.95839365  10.20302519 -13.1614264 ]\n",
      "-------------------:\n",
      "cumulative reward: 270.0\n",
      "CPU times: user 666 ms, sys: 5.93 ms, total: 672 ms\n",
      "Wall time: 680 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  2.95839365,  10.20302519, -13.1614264 ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent = RL_system(pacman_RL_environment())\n",
    "%time agent.learn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RL_system(object):\n",
    "    \n",
    "    def __init__(self, _environment):\n",
    "        self.environment = _environment\n",
    "        self.old_state = self.environment.getState()\n",
    "        self.learning_vector = self.initial_learning_vector() # the list of parameters to learn\n",
    "        self.eps_greedy = 0 # probability to play a random action\n",
    "        self.discount_factor = 0.9 # specifies how much long term reward is kept\n",
    "        self.learning_rate = 0.5\n",
    "        \n",
    "        \n",
    "        \n",
    "    def learn(self, iterations):\n",
    "        \"\"\"\n",
    "        Performs the learning steps a specified number of times.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            if(self.environment.game_is_over()):\n",
    "                break\n",
    "            self.learning_step()\n",
    "\n",
    "                \n",
    "        print(\"cumulative reward:\", self.environment.getCumulativeReward())\n",
    "            \n",
    "        return self.learning_vector\n",
    "    \n",
    "    \n",
    "    def learning_step(self):\n",
    "        \"\"\"\n",
    "        This is the Q-learning routine.\n",
    "        \"\"\"\n",
    "        \n",
    "        #s = self.environment.getState() # current state\n",
    "        s = self.old_state # calculated during the previous update_learning_vector(s,a,r)\n",
    "        #print(s)\n",
    "        a = self.policy(s) # action to perform according to the policy\n",
    "        print(\"action:\",a)\n",
    "        self.environment.perform_action(a)\n",
    "        r = self.environment.getReward() # gained reward\n",
    "        print(\"reward:\",r)\n",
    "        print(\"psi:\", self.action_state_features_vector(s,a))\n",
    "        self.update_learning_vector(s,a,r)\n",
    "        print(\"vector:\",self.learning_vector)\n",
    "        print(\"-------------------:\")\n",
    "    \n",
    "    \n",
    "    def policy(self, s):\n",
    "        \"\"\"\n",
    "        Returns the action to perform in the state s according to a policy.\n",
    "        \"\"\" \n",
    "        return self.best_Q_policy(s)\n",
    "    \n",
    "    \n",
    "    def Q(self, s,a):\n",
    "        \"\"\"\n",
    "        This is the Q function, it returns the expected future discounted reward\n",
    "        for taking action a ∈ A in state s ∈ S.\n",
    "        \"\"\"\n",
    "        return self.learning_vector.dot(self.action_state_features_vector(s,a))\n",
    "    \n",
    "    \n",
    "    def update_learning_vector(self, s,a,r):\n",
    "        \"\"\"\n",
    "        This function updates the learning vector.\n",
    "            a is the last performed action,\n",
    "            s is the previous state,\n",
    "            r is the reward that has been generated by performing a in state s. \"\"\"\n",
    "        \n",
    "        currentState = self.environment.getState()\n",
    "        max_Q = max([self.Q(currentState,action) for action in self.environment.getActions()])\n",
    "        difference = r + self.discount_factor*max_Q - self.Q(s,a)\n",
    "        self.learning_vector += self.learning_rate*difference*self.action_state_features_vector(s,a)\n",
    "        self.old_state = currentState\n",
    "         \n",
    "            \n",
    "    def best_Q_policy(self, s):\n",
    "        \"\"\"\n",
    "        For a given state It returns the action that maximize the Q_function, but it\n",
    "        can also return a random action with probability = eps_greedy. \n",
    "        \"\"\"\n",
    "        \n",
    "        actions = self.environment.getActions()\n",
    "        \n",
    "        if self.random_boolean(self.eps_greedy):\n",
    "            return np.random.choice(actions)\n",
    "        \n",
    "        i = np.argmax([self.Q(s,a) for a in actions])\n",
    "        return actions[i]\n",
    "    \n",
    "    \n",
    "    def random_boolean(self, probability_of_true):\n",
    "        \"\"\"\n",
    "        It returns true with the given probability, false otherwise.\n",
    "        \"\"\"\n",
    "        return np.random.random_sample()<probability_of_true\n",
    "    \n",
    "    \n",
    "    def initial_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the initial configuration of the learning vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.zeros(len(self.action_state_features_vector(self.old_state,self.environment.getActions()[0])))\n",
    "    \n",
    "    \n",
    "    def current_state_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the vector of numerical values representing the current state.\n",
    "        It basically extract the values from the state dictionary of the environment.\n",
    "        \"\"\"\n",
    "        return np.asarray(list(self.environment.getState().values()))\n",
    "    \n",
    "    \n",
    "    def action_state_features_vector(self, s, a):\n",
    "        \"\"\"\n",
    "        It returns a vector of numerical values representing relevant features of the state-action pair.\n",
    "        It basically extract the values from the psi(s,a) dictionary of the environment.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.asarray(list(self.environment.psi(s,a).values()))\n",
    "    \n",
    "    \n",
    "    def reset_environment(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        self.environment.restart()\n",
    "    \n",
    "    \n",
    "    def reset_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial predefined value.\n",
    "        \"\"\"\n",
    "        self.learning_vector = self.initial_learning_vector()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class RL_Environment(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getState(self):\n",
    "        \"\"\"\n",
    "        Should return the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Should return the list of all possible actions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def psi(self, s,a):\n",
    "        # maybe this function should belong to the RL_system, since what is relevant may depend\n",
    "        # on the learning procedure more than being an absolute property of the environment.\n",
    "        \"\"\"\n",
    "        Should return relevant features of the given state-action pair \n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Should return the last reward received.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a and it's state changes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class pacman_RL_environment(RL_Environment):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MsPacman-ram-v0')\n",
    "        \n",
    "        self.state = self.env.reset() # env ram representation of the current state\n",
    "        self.skip_intro() # the firsts steps you can't do anything, so it's better to skip them\n",
    "        self.current_reward = 0 # last reward received\n",
    "        self.cumulative_reward = 0\n",
    "        self.game_over = False\n",
    "        \n",
    "        # the features_extractor is here because it has (and need) a state\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen()) \n",
    "        \n",
    "        \n",
    "    def getState(self):\n",
    "        \"\"\" \n",
    "        Returns the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "\n",
    "        # some examples of state features \n",
    "        features = {} \n",
    "        \n",
    "        for entity in ['ghosts', 'foods', 'special_food']:\n",
    "            for movement in ['up', 'down' , 'right', 'left']:\n",
    "                features[\"nearest_\" + entity + \"_distance_after_going_\" + movement] \\\n",
    "                = self.features_extractor.nearest_entity_distance_from_pacman_after_movement(movement, entity)\n",
    "                \n",
    "        for entity in ['ghosts', 'foods', 'special_food']:\n",
    "                features[\"nearest_\" + entity + \"_distance\"] \\\n",
    "                = self.features_extractor.nearest_entity_distance_from_pacman(entity)\n",
    "                \n",
    "        #features[\"ghost_are_scared\"] = features_extractor.ghost_are_scared()\n",
    "        # features[\"actual_time_step\"] =\n",
    "        # features[\"last_scared_ghost_time_step\"] = \n",
    "        \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of all possible actions as strings.\n",
    "        \"\"\"\n",
    "        return list(self.actions_dict().keys())\n",
    "    \n",
    "    \n",
    "    def psi(self, s, a):\n",
    "        \"\"\"\n",
    "        Returns relevant features of the given state-action pair \n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        \n",
    "        # these are just examples taken from the paper\n",
    "        features = {}\n",
    "        \n",
    "        #for entity in ['ghosts', 'foods', 'special_food']:\n",
    "        #    features[\"distance_of_the_closest_\" + entity] \\\n",
    "        #    = s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "            \n",
    "        for entity in ['ghosts', 'foods', 'special_food']:\n",
    "            features[\"getting_closer_to\" + entity] \\\n",
    "            = s[\"nearest_\" + entity + \"_distance\"] - \\\n",
    "              s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "              \n",
    "            \n",
    "        #features[\"distance_of_the_closest_food\"] = distance_of_the_next_closest_food(s,a)\n",
    "        #features[\"distance_of_the_closest_ghost\"] = distance_of_the_closest_ghost(s,a)\n",
    "        \n",
    "        #features[\"food_will_be_eaten\"] = food_will_be_eaten(s,a)\n",
    "        #features[\"ghost_collision_is_possible\"] = ghost_collision_is_possible(s,a)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Returns the last reward received.\n",
    "        \"\"\"\n",
    "        return self.current_reward\n",
    "    \n",
    "    \n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a (given as a string) and it's state changes.\n",
    "        \"\"\"\n",
    "        \n",
    "        encoded_action = self.actions_dict()[a] # translate the action from string to number\n",
    "        self.state, self.current_reward, self.game_over, info = self.env.step(encoded_action)\n",
    "        self.cumulative_reward += self.current_reward\n",
    "        \n",
    "        # then we have to update the features extractor, \n",
    "        # since features extraction doesn't depend only on the current screen\n",
    "        self.features_extractor.update(self.getCurrentScreen())\n",
    "        \n",
    "    \n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state = self.env.reset()\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen())\n",
    "        \n",
    "    \n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        return self.game_over\n",
    "        \n",
    "        \n",
    "    def actions_dict(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of (action name - action encoded).\n",
    "        The encoding is needed to give the commands to the env.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions_d = {\"up\":1, \"down\":4, \"right\":2, \"left\":3}\n",
    "        return actions_d\n",
    "    \n",
    "    \n",
    "    def getCurrentScreen(self):\n",
    "        \"\"\"\n",
    "        Returns the current game screen.\n",
    "        \"\"\"\n",
    "        return self.env.env.ale.getScreen().reshape(210, 160)\n",
    "    \n",
    "    def skip_intro(self):\n",
    "        intro_duration = 90\n",
    "        for i in range(intro_duration):\n",
    "            self.env.step(1)\n",
    "            \n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        return self.cumulative_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pacman_features_extractor(object):\n",
    "\n",
    "    def __init__(self, initial_screen):\n",
    "        self.positions = {'pacman': None, 'ghosts': None, 'foods': None, 'special_food': None}\n",
    "        self.ghosts_scared = False\n",
    "        self.epsilon = 3\n",
    "        self.railwayGraph = RailwayGraph()\n",
    "        \n",
    "        self.initialize_foods()\n",
    "        self.update(initial_screen)\n",
    "        \n",
    "    def update(self, screen):\n",
    "        \"\"\"\n",
    "        Given a screen, it updates the position and state of all objects.\n",
    "        \"\"\"\n",
    "        self.update_guys(screen)\n",
    "        self.update_foods()\n",
    "\n",
    "    \n",
    "    ####### features #######\n",
    "    \n",
    "    def nearest_food_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman('foods')\n",
    "    \n",
    "    def nearest_ghost_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest ghost.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman('ghosts')\n",
    "    \n",
    "    def nearest_special_food_distance():\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest special food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman('special_food')\n",
    "    \n",
    "    def ghost_are_scared():\n",
    "        \"\"\"\n",
    "        Return true if ghosts are scared, false otherwise.\n",
    "        \"\"\"\n",
    "        return self.ghosts_scared                \n",
    "\n",
    "    \n",
    "    def update_guys(self, screen):\n",
    "        self.update_ghosts_scared(screen)\n",
    "        raw_guys_positions = self.extract_raw_guys_positions(screen)\n",
    "        self.positions['pacman'] = self.railwayGraph.nearest_node_to_pixel(raw_guys_positions['pacman'])\n",
    "        \n",
    "        # case 1: ghost are visible (scared or not)\n",
    "        if(raw_guys_positions['ghosts']!= []): # I hope this is the right condition\n",
    "            #for pos in (raw_guys_positions['ghosts']):\n",
    "            self.positions['ghosts'] = [self.railwayGraph.nearest_node_to_pixel(pos) for pos in raw_guys_positions['ghosts']]\n",
    "                \n",
    "        # case 2: ghost are not visible -> just do nothing, we keep the old positions\n",
    "                \n",
    "            \n",
    "    def initialize_foods(self):\n",
    "        \"\"\"\n",
    "        It set the initial position of every food based on a-priori knowledge.\n",
    "        \"\"\"\n",
    "        \n",
    "        foods_list = self.food_initial_raw_positions()\n",
    "        foods_nodes = []\n",
    "        for food_pos in foods_list:\n",
    "            foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(food_pos))\n",
    "        self.positions['foods'] = foods_nodes\n",
    "        \n",
    "        sp_foods_list = self.sp_food_initial_raw_positions()\n",
    "        sp_foods_nodes = []\n",
    "        for sp_food_pos in sp_foods_list:\n",
    "            sp_foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(sp_food_pos))\n",
    "        self.positions['special_food'] = sp_foods_nodes\n",
    "        \n",
    "            \n",
    "    def update_foods(self):\n",
    "        foods_distances = np.asarray([np.linalg.norm(np.asarray(food_pos) - np.asarray(self.positions['pacman']), ord=1)\n",
    "                           for food_pos in self.positions['foods']])\n",
    "\n",
    "        if np.min(foods_distances) < self.epsilon:\n",
    "            self.positions['foods'].pop(np.argmin(foods_distances))\n",
    "            \n",
    "        sp_foods_distances = np.asarray([np.linalg.norm(np.asarray(sp_food_pos) - np.asarray(self.positions['pacman']), ord=1)\n",
    "                           for sp_food_pos in self.positions['special_food']])\n",
    "\n",
    "        if np.min(sp_foods_distances) < self.epsilon:\n",
    "            self.positions['special_food'].pop(np.argmin(sp_foods_distances))\n",
    "            \n",
    "            \n",
    "    def nearest_entity_distance_from_pacman(self, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity.\n",
    "        \"\"\"\n",
    "        \n",
    "        beast_pos = self.positions['pacman']\n",
    "        entity_positions = self.positions[entity_name]\n",
    "        return min([self.railwayGraph.get_distance(beast_pos, e_pos) for e_pos in entity_positions])\n",
    "    \n",
    "    \n",
    "    def nearest_entity_distance_from_pacman_after_movement(self, movement, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity after a given movement of the beast.\n",
    "        \"\"\"\n",
    "        \n",
    "        beast_pos = self.positions['pacman']\n",
    "        entity_positions = self.positions[entity_name]  \n",
    "        return min([self.railwayGraph.get_distance_after_source_movement(beast_pos, movement, e_pos) for e_pos in entity_positions])\n",
    "\n",
    "    \n",
    "    def update_ghosts_scared(self, screen):\n",
    "        \"\"\"\n",
    "        Update ghosts_scared variable according to the given screen.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ghosts_scared = False \n",
    "    \n",
    "    \n",
    "    def extract_raw_guys_positions(self, screen):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with the raw positions of all 'guys', extracted from the given screen.\n",
    "        \"\"\"\n",
    "        guys_pos = self.PacmanAndGhostsCoords(screen)\n",
    "        return {'pacman' : guys_pos[0], 'ghosts' : guys_pos[1]}\n",
    "    \n",
    "    \n",
    "    def food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/food_coords.npy\"))\n",
    "    \n",
    "    \n",
    "    def sp_food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/special_food_coords.npy\"))\n",
    "        \n",
    "        \n",
    "    def center(self, SpecificMatrix):\n",
    "        \"\"\"\n",
    "        Given a matrix with 1 where lies the object you want to detect and 0 elsewhere,\n",
    "        the position of the center of the object is returned.\n",
    "        \"\"\"\n",
    "        a = np.where(SpecificMatrix == 1)\n",
    "        y = a[0]\n",
    "        x = a[1]\n",
    "\n",
    "        x_bar = (x.max() + x.min())/2\n",
    "        y_bar = (y.max() + y.min())/2\n",
    "\n",
    "        return (x_bar, y_bar)\n",
    "\n",
    "\n",
    "    def find_location(self, screen, value):\n",
    "        \"\"\"\n",
    "        Find the object corresponding to value within the matrix. If it is not present None is returned.\n",
    "        \"\"\"\n",
    "        SpecificMatrix = (screen == value).astype(int)\n",
    "        if SpecificMatrix.sum() == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return self.center(SpecificMatrix)\n",
    "\n",
    "        \n",
    "    def PacmanAndGhostsCoords(self, screen, PacmanValue=42, WallsFoodValue=74, GhostsValues=[70, 38, 184, 88], ghosts_scared=False):\n",
    "        \"\"\"\n",
    "        Given the matrix of the screen, a list with the positions of all the relevant objects is returned.\n",
    "        \"\"\"\n",
    "        pacman_coords = self.find_location(screen, PacmanValue)\n",
    "\n",
    "        if ghosts_scared:\n",
    "            pass\n",
    "        else:\n",
    "            ghosts_coords = []\n",
    "            for ghost_value in GhostsValues:\n",
    "                location = self.find_location(screen, ghost_value)\n",
    "                if(location!=None):\n",
    "                    ghosts_coords.append(self.find_location(screen, ghost_value))\n",
    "\n",
    "        return [pacman_coords, ghosts_coords]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx # great library that implements a lot of useful (and efficient) data structures and algorithms for graphs\n",
    "\n",
    "\n",
    "class RailwayGraph(object):\n",
    "    '''\n",
    "    Graph of all the pixels which can be crossed at any time by Pacman.\n",
    "    In order to be a node, a pixel needs to lie in a corridor. Each different pixel\n",
    "    can be connected to any of the four Von Neumann neighbors (up, right, down, left), provided \n",
    "    they are admissible. The NetworkX library is used.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.initialize_graph()\n",
    "        self.directions = {'up' : (-1, 0), 'down' : (1, 0), 'right' : (0, 1), 'left' : (0, -1)}\n",
    "   \n",
    "\n",
    "    def initialize_graph(self):\n",
    "        # ce lo salviamo\n",
    "        '''\n",
    "        Initializes the graph. It loop over every corridor pixel over the rails_map matrix and adds \n",
    "        its corresponding node. For each new node, the presence of neighbors is checked, and they are \n",
    "        eventually added. Finally, those pixel who are not\n",
    "        '''\n",
    "        \n",
    "        self.graph = nx.Graph() # NetworkX-provided data structure to represent a graph\n",
    "        rails_map = np.load(\"saved_objects/rails_matrix.npy\").astype(int)\n",
    "        corridors_color = 1\n",
    "        m, n = np.shape(rails_map)\n",
    "        \n",
    "        # Loop over all pixels\n",
    "        for row in range(m):\n",
    "            for col in range(n):\n",
    "                color = rails_map[row, col]\n",
    "                if (color == corridors_color) and ((row, col) not in self.graph.nodes): # if corridor\n",
    "                    self.graph.add_node((row, col)) #add node to the graph\n",
    "                    \n",
    "                    # Loop over the neighbors and establish edge if necessary\n",
    "                    for offset in [(0, 1), (0, -1), (-1, 0), (1, 0)]:\n",
    "                        neighbor = (row + offset[0], col + offset[1])\n",
    "                        \n",
    "                        if (0 <= neighbor[0] <= m-1) \\\n",
    "                        and (0 <= neighbor[1] <= n-1) \\\n",
    "                        and (rails_map[neighbor[0], neighbor[1]] == corridors_color): # if not out of bounds and colored appropriately\n",
    "                            self.graph.add_edge((row, col), neighbor)\n",
    "            \n",
    "            \n",
    "    def nearest_node_to_pixel(self, pixel_coords):\n",
    "        \"\"\"\n",
    "        This function looks for the node which has the closest key to the given pixel (L1 distance)\n",
    "        and returns a tuple with its coordinates.\n",
    "        \"\"\"\n",
    "        nodes_arr = np.asarray(self.graph.nodes())\n",
    "        closest_node = nodes_arr[np.argmin(np.linalg.norm(nodes_arr - pixel_coords, ord=1, axis=1))]\n",
    "        return tuple(closest_node)\n",
    "    \n",
    "    \n",
    "    def get_distance(self, source, target):\n",
    "        '''\n",
    "        Computes the shortest distance from source to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        '''\n",
    "        return nx.shortest_path_length(self.graph, source, target)\n",
    "    \n",
    "    def get_distance_after_source_movement(self, source, movement, target):\n",
    "        '''\n",
    "        Computes the shortest distance from the source after a given movement to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        '''\n",
    "        return nx.shortest_path_length(self.graph, self.nextNode(source, movement), target)\n",
    "\n",
    "    \n",
    "    def nextNode(self, node, movement):\n",
    "        \"\"\"\n",
    "        Returns the node of the graph reached performing a step in a given direction from a given node.\n",
    "        \"\"\"\n",
    "        possibleMovements = self.getPossibleMovements(node)\n",
    "        if movement in possibleMovements:\n",
    "            return (node[0] + self.directions[movement][0], node[1] + self.directions[movement][1])\n",
    "        else:\n",
    "            return node\n",
    "    \n",
    "    \n",
    "    def getPossibleMovements(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of possible movements (up, down, right, left) that can be done starting\n",
    "        from a given node of the graph.\n",
    "        \"\"\"\n",
    "        neighbours = self.getNeighbours(node)\n",
    "        possibleMovements = []\n",
    "        for movement in self.directions.keys():\n",
    "            newNode = (node[0] + self.directions[movement][0], node[1] + self.directions[movement][1])\n",
    "            if (newNode in neighbours):\n",
    "                possibleMovements.append(movement)\n",
    "        return possibleMovements\n",
    "    \n",
    "    \n",
    "    def getNeighbours(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of neighbors of a given node in the graph.\n",
    "        \"\"\"\n",
    "        neighbours = []\n",
    "        for neighbour in nx.all_neighbors(self.graph, node):\n",
    "            neighbours.append(neighbour)\n",
    "        return neighbours\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
