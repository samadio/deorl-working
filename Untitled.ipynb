{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import numpy as np                   # tensor maths\n",
    "import networkx as nx                # operations on graphs\n",
    "import gym                           # RL environment\n",
    "from abc import ABC, abstractmethod  # interface verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRAPH MACHINERY\n",
    "\n",
    "class RailwayGraph():\n",
    "    \"\"\"\n",
    "    Graph of all the pixels which can be crossed at any time by Pacman.\n",
    "    In order to be a node, a pixel needs to lie in a corridor. Each different pixel\n",
    "    can be connected to any of the four Von Neumann neighbors (up, right, down, left), provided\n",
    "    they are admissible. The NetworkX library is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.initialize_graph()\n",
    "        self.directions = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"right\": (0, 1),\n",
    "            \"left\": (0, -1),\n",
    "        }\n",
    "\n",
    "    def initialize_graph(self):\n",
    "        # The graph is initialized and saved\n",
    "        \"\"\"\n",
    "        Initializes the graph. It loop over every corridor pixel over the rails_map matrix and adds\n",
    "        its corresponding node. For each new node, the presence of neighbors is checked, and they are\n",
    "        eventually added. Finally, those pixel who are not\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = nx.Graph()  # NetworkX-provided data structure to represent a graph\n",
    "        rails_map = np.load(\"saved_objects/rails_matrix.npy\").astype(int)\n",
    "        corridors_color = 1\n",
    "        m, n = np.shape(rails_map)\n",
    "\n",
    "        # Loop over all pixels\n",
    "        for row in range(m):\n",
    "            for col in range(n):\n",
    "                color = rails_map[row, col]\n",
    "                if (color == corridors_color) and (\n",
    "                    (row, col) not in self.graph.nodes\n",
    "                ):  # if corridor\n",
    "                    self.graph.add_node((row, col))  # add node to the graph\n",
    "\n",
    "                    # Loop over the neighbors and establish edge if necessary\n",
    "                    for offset in [(0, 1), (0, -1), (-1, 0), (1, 0)]:\n",
    "                        neighbor = (row + offset[0], col + offset[1])\n",
    "\n",
    "                        if (\n",
    "                            (0 <= neighbor[0] <= m - 1)\n",
    "                            and (0 <= neighbor[1] <= n - 1)\n",
    "                            and (rails_map[neighbor[0], neighbor[1]] == corridors_color)\n",
    "                        ):  # if not out of bounds and colored appropriately\n",
    "                            self.graph.add_edge((row, col), neighbor)\n",
    "\n",
    "    def nearest_node_to_pixel(self, pixel_coords):\n",
    "        \"\"\"\n",
    "        This function looks for the node which has the closest key to the given pixel (L1 distance)\n",
    "        and returns a tuple with its coordinates.\n",
    "        \"\"\"\n",
    "        nodes_arr = np.asarray(self.graph.nodes())\n",
    "        closest_node = nodes_arr[\n",
    "            np.argmin(np.linalg.norm(nodes_arr - pixel_coords, ord=1, axis=1))\n",
    "        ]\n",
    "        return tuple(closest_node)\n",
    "\n",
    "    def get_distance(self, source, target):\n",
    "        \"\"\"\n",
    "        Computes the shortest distance from source to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        \"\"\"\n",
    "        return nx.shortest_path_length(self.graph, source, target)\n",
    "\n",
    "    def get_distance_after_source_movement(self, source, movement, target):\n",
    "        \"\"\"\n",
    "        Computes the shortest distance from the source after a given movement to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        \"\"\"\n",
    "        return nx.shortest_path_length(\n",
    "            self.graph, self.nextNode(source, movement), target\n",
    "        )\n",
    "\n",
    "    def nextNode(self, node, movement):\n",
    "        \"\"\"\n",
    "        Returns the node of the graph reached performing a step in a given direction from a given node.\n",
    "        \"\"\"\n",
    "        possibleMovements = self.getPossibleMovements(node)\n",
    "        if movement in possibleMovements:\n",
    "            return (\n",
    "                node[0] + self.directions[movement][0],\n",
    "                node[1] + self.directions[movement][1],\n",
    "            )\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    def getPossibleMovements(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of possible movements (up, down, right, left) that can be done starting\n",
    "        from a given node of the graph.\n",
    "        \"\"\"\n",
    "        neighbours = self.getNeighbours(node)\n",
    "        possibleMovements = []\n",
    "        for movement in self.directions.keys():\n",
    "            newNode = (\n",
    "                node[0] + self.directions[movement][0],\n",
    "                node[1] + self.directions[movement][1],\n",
    "            )\n",
    "            if newNode in neighbours:\n",
    "                possibleMovements.append(movement)\n",
    "        return possibleMovements\n",
    "\n",
    "    def getNeighbours(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of neighbors of a given node in the graph.\n",
    "        \"\"\"\n",
    "        neighbours = []\n",
    "        for neighbour in nx.all_neighbors(self.graph, node):\n",
    "            neighbours.append(neighbour)\n",
    "        return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTOR (gym frame -> clever representation)\n",
    "\n",
    "class Pacman_features_extractor():\n",
    "    def __init__(self, initial_screen):\n",
    "        self.positions = {\n",
    "            \"pacman\": None,\n",
    "            \"ghosts\": None,\n",
    "            \"foods\": None,\n",
    "            \"special_food\": None,\n",
    "        }\n",
    "        self.ghosts_scared = False\n",
    "        self.epsilon = 3\n",
    "        self.railwayGraph = RailwayGraph()\n",
    "\n",
    "        self.initialize_foods()\n",
    "        self.update(initial_screen)\n",
    "\n",
    "    def update(self, screen):\n",
    "        \"\"\"\n",
    "        Given a screen, it updates the position and state of all objects.\n",
    "        \"\"\"\n",
    "        self.update_guys(screen)\n",
    "        self.update_foods()\n",
    "\n",
    "    # List of features |->\n",
    "\n",
    "    def nearest_food_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"foods\")\n",
    "\n",
    "    def nearest_ghost_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest ghost.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"ghosts\")\n",
    "\n",
    "    def nearest_special_food_distance():\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest special food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"special_food\")\n",
    "\n",
    "    def ghost_are_scared():\n",
    "        \"\"\"\n",
    "        Return true if ghosts are scared, false otherwise.\n",
    "        \"\"\"\n",
    "        return self.ghosts_scared\n",
    "    # <-| List of features (end)\n",
    "\n",
    "    \n",
    "    # Classes for features initialization/update/passing\n",
    "    \n",
    "    def nearest_entity_distance_from_pacman(self, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity.\n",
    "        \"\"\"\n",
    "\n",
    "        beast_pos = self.positions[\"pacman\"]\n",
    "        entity_positions = self.positions[entity_name]\n",
    "        return min(\n",
    "            [\n",
    "                self.railwayGraph.get_distance(beast_pos, e_pos)\n",
    "                for e_pos in entity_positions\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def nearest_entity_distance_from_pacman_after_movement(self, movement, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity after a given movement of the beast.\n",
    "        \"\"\"\n",
    "\n",
    "        beast_pos = self.positions[\"pacman\"]\n",
    "        entity_positions = self.positions[entity_name]\n",
    "        return min(\n",
    "            [\n",
    "                self.railwayGraph.get_distance_after_source_movement(\n",
    "                    beast_pos, movement, e_pos\n",
    "                )\n",
    "                for e_pos in entity_positions\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def initialize_foods(self):\n",
    "        \"\"\"\n",
    "        It set the initial position of every food based on a-priori knowledge.\n",
    "        \"\"\"\n",
    "\n",
    "        foods_list = self.food_initial_raw_positions()\n",
    "        foods_nodes = []\n",
    "        for food_pos in foods_list:\n",
    "            foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(food_pos))\n",
    "        self.positions[\"foods\"] = foods_nodes\n",
    "\n",
    "        sp_foods_list = self.sp_food_initial_raw_positions()\n",
    "        sp_foods_nodes = []\n",
    "        for sp_food_pos in sp_foods_list:\n",
    "            sp_foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(sp_food_pos))\n",
    "        self.positions[\"special_food\"] = sp_foods_nodes\n",
    "\n",
    "    def update_foods(self):\n",
    "        foods_distances = np.asarray(\n",
    "            [\n",
    "                np.linalg.norm(\n",
    "                    np.asarray(food_pos) - np.asarray(self.positions[\"pacman\"]), ord=1\n",
    "                )\n",
    "                for food_pos in self.positions[\"foods\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if np.min(foods_distances) < self.epsilon:\n",
    "            self.positions[\"foods\"].pop(np.argmin(foods_distances))\n",
    "\n",
    "        sp_foods_distances = np.asarray(\n",
    "            [\n",
    "                np.linalg.norm(\n",
    "                    np.asarray(sp_food_pos) - np.asarray(self.positions[\"pacman\"]),\n",
    "                    ord=1,\n",
    "                )\n",
    "                for sp_food_pos in self.positions[\"special_food\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if np.min(sp_foods_distances) < self.epsilon:\n",
    "            self.positions[\"special_food\"].pop(np.argmin(sp_foods_distances))\n",
    "\n",
    "\n",
    "    def update_ghosts_scared(self, screen):\n",
    "        \"\"\"\n",
    "        Update ghosts_scared variable according to the given screen.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ghosts_scared = False\n",
    "        \n",
    "        \n",
    "    def center(self, SpecificMatrix):\n",
    "        \"\"\"\n",
    "        Given a matrix with 1 where lies the object you want to detect and 0 elsewhere,\n",
    "        the position of the center of the object is returned.\n",
    "        \"\"\"\n",
    "        a = np.where(SpecificMatrix == 1)\n",
    "        y = a[0]\n",
    "        x = a[1]\n",
    "\n",
    "        x_bar = (x.max() + x.min()) / 2\n",
    "        y_bar = (y.max() + y.min()) / 2\n",
    "\n",
    "        return (x_bar, y_bar)        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def find_location(self, screen, value):\n",
    "        \"\"\"\n",
    "        Find the object corresponding to value within the matrix. If it is not present None is returned.\n",
    "        \"\"\"\n",
    "        SpecificMatrix = (screen == value).astype(int)\n",
    "        if SpecificMatrix.sum() == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return self.center(SpecificMatrix)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def PacmanAndGhostsCoords(\n",
    "        self,\n",
    "        screen,\n",
    "        PacmanValue=42,\n",
    "        WallsFoodValue=74,\n",
    "        GhostsValues=[70, 38, 184, 88],\n",
    "        ghosts_scared=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Given the matrix of the screen, a list with the positions of all the relevant objects is returned.\n",
    "        \"\"\"\n",
    "        pacman_coords = self.find_location(screen, PacmanValue)\n",
    "\n",
    "        if ghosts_scared:\n",
    "            pass\n",
    "        else:\n",
    "            ghosts_coords = []\n",
    "            for ghost_value in GhostsValues:\n",
    "                location = self.find_location(screen, ghost_value)\n",
    "                if location != None:\n",
    "                    ghosts_coords.append(self.find_location(screen, ghost_value))\n",
    "\n",
    "        return [pacman_coords, ghosts_coords]\n",
    "\n",
    "    \n",
    "    \n",
    "    def extract_raw_guys_positions(self, screen):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with the raw positions of all 'guys', extracted from the given screen.\n",
    "        \"\"\"\n",
    "        guys_pos = self.PacmanAndGhostsCoords(screen)\n",
    "        return {\"pacman\": guys_pos[0], \"ghosts\": guys_pos[1]}\n",
    "\n",
    "    def food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/food_coords.npy\"))\n",
    "\n",
    "    def sp_food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/special_food_coords.npy\"))\n",
    "    \n",
    "    def update_guys(self, screen):\n",
    "        self.update_ghosts_scared(screen)\n",
    "        raw_guys_positions = self.extract_raw_guys_positions(screen)\n",
    "        self.positions[\"pacman\"] = self.railwayGraph.nearest_node_to_pixel(\n",
    "            raw_guys_positions[\"pacman\"]\n",
    "        )\n",
    "\n",
    "        # case 1: ghost are visible (scared or not)\n",
    "        if raw_guys_positions[\"ghosts\"] != []:  # I hope this is the right condition\n",
    "            # for pos in (raw_guys_positions['ghosts']):\n",
    "            self.positions[\"ghosts\"] = [\n",
    "                self.railwayGraph.nearest_node_to_pixel(pos)\n",
    "                for pos in raw_guys_positions[\"ghosts\"]\n",
    "            ]\n",
    "\n",
    "        # case 2: ghost are not visible -> just do nothing, we keep the old positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERFACE VERIFICATION\n",
    "\n",
    "class RL_Environment(ABC):\n",
    "    @abstractmethod\n",
    "    def getState(self):\n",
    "        \"\"\"\n",
    "        Should return the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Should return the list of all possible actions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Should return the last reward received.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a and it's state changes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def psi(self, s, a):\n",
    "        \"\"\"\n",
    "        Should return relevant features of the given state-action pair\n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "\n",
    "#class RL_system(ABC):\n",
    "#    @abstractmethod\n",
    "#    def psi(self, s, a):\n",
    "#        \"\"\"\n",
    "#        Should return relevant features of the given state-action pair\n",
    "#        as a dictionary of (feature name - feature value).\n",
    "#        \"\"\"\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL ENVIRONMENT (pt. 1)\n",
    "\n",
    "class pacman_RL_environment(RL_Environment):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"MsPacman-ram-v0\")\n",
    "\n",
    "        self.state = self.env.reset()  # env ram representation of the current state\n",
    "        self.skip_intro()  # the firsts steps you can't do anything, so it's better to skip them\n",
    "        self.current_reward = 0  # last reward received\n",
    "        self.cumulative_reward = 0\n",
    "        self.game_over = False\n",
    "\n",
    "        # the features_extractor is here because it has (and need) a state\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen())\n",
    "\n",
    "    def getState(self):\n",
    "        \"\"\"\n",
    "        Returns the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "\n",
    "        # some examples of state features\n",
    "        features = {}\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            for movement in [\"up\", \"down\", \"right\", \"left\"]:\n",
    "                features[\n",
    "                    \"nearest_\" + entity + \"_distance_after_going_\" + movement\n",
    "                ] = self.features_extractor.nearest_entity_distance_from_pacman_after_movement(\n",
    "                    movement, entity\n",
    "                )\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            features[\n",
    "                \"nearest_\" + entity + \"_distance\"\n",
    "            ] = self.features_extractor.nearest_entity_distance_from_pacman(entity)\n",
    "\n",
    "        # features[\"ghost_are_scared\"] = features_extractor.ghost_are_scared()\n",
    "        # features[\"actual_time_step\"] =\n",
    "        # features[\"last_scared_ghost_time_step\"] =\n",
    "\n",
    "        return features\n",
    "\n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of all possible actions as strings.\n",
    "        \"\"\"\n",
    "        return list(self.actions_dict().keys())\n",
    "    \n",
    "    def psi(self, s, a):\n",
    "        \"\"\"\n",
    "        Returns relevant features of the given state-action pair\n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "\n",
    "        # these are just examples taken from the paper\n",
    "        features = {}\n",
    "\n",
    "        # for entity in ['ghosts', 'foods', 'special_food']:\n",
    "        #    features[\"distance_of_the_closest_\" + entity] \\\n",
    "        #    = s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            features[\"getting_closer_to\" + entity] = (\n",
    "                s[\"nearest_\" + entity + \"_distance\"]\n",
    "                - s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "            )\n",
    "\n",
    "        # features[\"distance_of_the_closest_food\"] = distance_of_the_next_closest_food(s,a)\n",
    "        # features[\"distance_of_the_closest_ghost\"] = distance_of_the_closest_ghost(s,a)\n",
    "        # features[\"food_will_be_eaten\"] = food_will_be_eaten(s,a)\n",
    "        # features[\"ghost_collision_is_possible\"] = ghost_collision_is_possible(s,a)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Returns the last reward received.\n",
    "        \"\"\"\n",
    "        return self.current_reward\n",
    "\n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a (given as a string) and it's state changes.\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_action = self.actions_dict()[\n",
    "            a\n",
    "        ]  # translate the action from string to number\n",
    "        self.state, self.current_reward, self.game_over, info = self.env.step(\n",
    "            encoded_action\n",
    "        )\n",
    "        self.cumulative_reward += self.current_reward\n",
    "\n",
    "        # then we have to update the features extractor,\n",
    "        # since features extraction doesn't depend only on the current screen\n",
    "        self.features_extractor.update(self.getCurrentScreen())\n",
    "\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state = self.env.reset()\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen())\n",
    "\n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        return self.game_over\n",
    "\n",
    "    def actions_dict(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of (action name - action encoded).\n",
    "        The encoding is needed to give the commands to the env.\n",
    "        \"\"\"\n",
    "\n",
    "        actions_d = {\"up\": 1, \"down\": 4, \"right\": 2, \"left\": 3}\n",
    "        return actions_d\n",
    "\n",
    "    def getCurrentScreen(self):\n",
    "        \"\"\"\n",
    "        Returns the current game screen.\n",
    "        \"\"\"\n",
    "        return self.env.env.ale.getScreen().reshape(210, 160)\n",
    "\n",
    "    def skip_intro(self):\n",
    "        intro_duration = 90\n",
    "        for i in range(intro_duration):\n",
    "            self.env.step(1)\n",
    "\n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        return self.cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL ENVIRONMENT (pt. 2)\n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "# In that case, mind to comment the other class declaration.\n",
    "\n",
    "#class pacman_RL_system(RL_system):\n",
    "\n",
    "class RL_system():\n",
    "    def __init__(self, _environment):\n",
    "        self.environment = _environment\n",
    "        self.old_state = self.environment.getState()\n",
    "        self.learning_vector = (\n",
    "            self.initial_learning_vector()\n",
    "        )  # the list of parameters to learn\n",
    "        self.eps_greedy = 0  # probability to play a random action\n",
    "        self.discount_factor = 0.9  # specifies how much long term reward is kept\n",
    "        self.learning_rate = 0.5\n",
    "        \n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "\n",
    "    #def psi(self, s, a):\n",
    "    #    \"\"\"\n",
    "    #    Returns relevant features of the given state-action pair\n",
    "    #    as a dictionary of (feature name - feature value).\n",
    "    #    \"\"\"\n",
    "    #\n",
    "    #    ## these are just examples taken from the paper\n",
    "    #    features = {}\n",
    "    #\n",
    "    #    ## for entity in ['ghosts', 'foods', 'special_food']:\n",
    "    #    ##    features[\"distance_of_the_closest_\" + entity] \\\n",
    "    #    ##    = s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "    #\n",
    "    #    for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "    #        features[\"getting_closer_to\" + entity] = (\n",
    "    #            s[\"nearest_\" + entity + \"_distance\"]\n",
    "    #            - s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "    #        )\n",
    "    #\n",
    "    #    ## features[\"distance_of_the_closest_food\"] = distance_of_the_next_closest_food(s,a)\n",
    "    #    ## features[\"distance_of_the_closest_ghost\"] = distance_of_the_closest_ghost(s,a)\n",
    "    #    ## features[\"food_will_be_eaten\"] = food_will_be_eaten(s,a)\n",
    "    #    ## features[\"ghost_collision_is_possible\"] = ghost_collision_is_possible(s,a)\n",
    "    #\n",
    "    #    return features\n",
    "\n",
    "    def learn(self, iterations):\n",
    "        \"\"\"\n",
    "        Performs the learning steps a specified number of times.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(iterations):\n",
    "            if self.environment.game_is_over():\n",
    "                break\n",
    "            self.learning_step()\n",
    "\n",
    "        print(\"cumulative reward:\", self.environment.getCumulativeReward())\n",
    "\n",
    "        return self.learning_vector\n",
    "\n",
    "    def learning_step(self):\n",
    "        \"\"\"\n",
    "        This is the Q-learning routine.\n",
    "        \"\"\"\n",
    "\n",
    "        # s = self.environment.getState() # current state\n",
    "        s = (\n",
    "            self.old_state\n",
    "        )  # calculated during the previous update_learning_vector(s,a,r)\n",
    "        # print(s)\n",
    "        a = self.policy(s)  # action to perform according to the policy\n",
    "        print(\"action:\", a)\n",
    "        self.environment.perform_action(a)\n",
    "        r = self.environment.getReward()  # gained reward\n",
    "        print(\"reward:\", r)\n",
    "        print(\"psi:\", self.action_state_features_vector(s, a))\n",
    "        self.update_learning_vector(s, a, r)\n",
    "        print(\"vector:\", self.learning_vector)\n",
    "        print(\"-------------------:\")\n",
    "        \n",
    "\n",
    "    def policy(self, s):\n",
    "        \"\"\"\n",
    "        Returns the action to perform in the state s according to a policy.\n",
    "        \"\"\"\n",
    "        return self.best_Q_policy(s)\n",
    "\n",
    "    def Q(self, s, a):\n",
    "        \"\"\"\n",
    "        This is the Q function, it returns the expected future discounted reward\n",
    "        for taking action a ∈ A in state s ∈ S.\n",
    "        \"\"\"\n",
    "        return self.learning_vector.dot(self.action_state_features_vector(s, a))\n",
    "\n",
    "    def update_learning_vector(self, s, a, r):\n",
    "        \"\"\"\n",
    "        This function updates the learning vector.\n",
    "            a is the last performed action,\n",
    "            s is the previous state,\n",
    "            r is the reward that has been generated by performing a in state s. \"\"\"\n",
    "\n",
    "        currentState = self.environment.getState()\n",
    "        max_Q = max(\n",
    "            [self.Q(currentState, action) for action in self.environment.getActions()]\n",
    "        )\n",
    "        difference = r + self.discount_factor * max_Q - self.Q(s, a)\n",
    "        self.learning_vector += (\n",
    "            self.learning_rate * difference * self.action_state_features_vector(s, a)\n",
    "        )\n",
    "        self.old_state = currentState\n",
    "\n",
    "    def best_Q_policy(self, s):\n",
    "        \"\"\"\n",
    "        For a given state It returns the action that maximize the Q_function, but it\n",
    "        can also return a random action with probability = eps_greedy.\n",
    "        \"\"\"\n",
    "\n",
    "        actions = self.environment.getActions()\n",
    "\n",
    "        if self.random_boolean(self.eps_greedy):\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "        i = np.argmax([self.Q(s, a) for a in actions])\n",
    "        return actions[i]\n",
    "\n",
    "    def random_boolean(self, probability_of_true):\n",
    "        \"\"\"\n",
    "        It returns true with the given probability, false otherwise.\n",
    "        \"\"\"\n",
    "        return np.random.random_sample() < probability_of_true\n",
    "\n",
    "    def initial_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the initial configuration of the learning vector.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.zeros(\n",
    "            len(\n",
    "                self.action_state_features_vector(\n",
    "                    self.old_state, self.environment.getActions()[0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def current_state_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the vector of numerical values representing the current state.\n",
    "        It basically extract the values from the state dictionary of the environment.\n",
    "        \"\"\"\n",
    "        return np.asarray(list(self.environment.getState().values()))\n",
    "\n",
    "    def action_state_features_vector(self, s, a):\n",
    "        \"\"\"\n",
    "        It returns a vector of numerical values representing relevant features of the state-action pair.\n",
    "        It basically extract the values from the psi(s,a) dictionary of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.asarray(list(self.environment.psi(s, a).values()))\n",
    "\n",
    "    def reset_environment(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        self.environment.restart()\n",
    "\n",
    "    def reset_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial predefined value.\n",
    "        \"\"\"\n",
    "        self.learning_vector = self.initial_learning_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/envs/RLpaper/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE AGENT\n",
    "\n",
    "agent = RL_system(pacman_RL_environment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: up\n",
      "reward: 10.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b00785cac384>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_is_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cumulative reward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCumulativeReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b00785cac384>\u001b[0m in \u001b[0;36mlearning_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"psi:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_state_features_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_learning_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vector:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b00785cac384>\u001b[0m in \u001b[0;36mupdate_learning_vector\u001b[0;34m(self, s, a, r)\u001b[0m\n\u001b[1;32m    100\u001b[0m             r is the reward that has been generated by performing a in state s. \"\"\"\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mcurrentState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         max_Q = max(\n\u001b[1;32m    104\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetActions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-e2d2ed262391>\u001b[0m in \u001b[0;36mgetState\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0;34m\"nearest_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_distance_after_going_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnearest_entity_distance_from_pacman_after_movement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mmovement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-171b7a850154>\u001b[0m in \u001b[0;36mnearest_entity_distance_from_pacman_after_movement\u001b[0;34m(self, movement, entity_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mbeast_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 )\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0me_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             ]\n\u001b[1;32m     83\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-10-171b7a850154>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mbeast_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 )\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0me_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             ]\n\u001b[1;32m     83\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-7-564652cb674d>\u001b[0m in \u001b[0;36mget_distance_after_source_movement\u001b[0;34m(self, source, movement, target)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         return nx.shortest_path_length(\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLpaper/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/generic.py\u001b[0m in \u001b[0;36mshortest_path_length\u001b[0;34m(G, source, target, weight, method)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Find shortest source-target path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'unweighted'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional_shortest_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dijkstra'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLpaper/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36mbidirectional_shortest_path\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# call helper to do the real work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bidirectional_pred_succ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLpaper/lib/python3.6/site-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_bidirectional_pred_succ\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mforward_fringe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis_level\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGsucc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                         \u001b[0mforward_fringe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RLpaper/lib/python3.6/site-packages/networkx/classes/coreviews.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAtlasView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DO STUFF\n",
    "\n",
    "iterations: int = 200\n",
    "    \n",
    "try:\n",
    "    get_ipython().run_line_magic(\"time\", \"agent.learn(iterations)\")\n",
    "except NameError:\n",
    "    agent.learn(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
