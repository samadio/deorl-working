{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.287321Z",
     "start_time": "2019-03-01T04:16:44.156611Z"
    }
   },
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import numpy as np                   # tensor maths\n",
    "import networkx as nx                # operations on graphs\n",
    "import gym                           # RL environment\n",
    "from abc import ABC, abstractmethod  # interface verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.304251Z",
     "start_time": "2019-03-01T04:16:45.289261Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRAPH MACHINERY\n",
    "\n",
    "class RailwayGraph():\n",
    "    \"\"\"\n",
    "    Graph of all the pixels which can be crossed at any time by Pacman.\n",
    "    In order to be a node, a pixel needs to lie in a corridor. Each different pixel\n",
    "    can be connected to any of the four Von Neumann neighbors (up, right, down, left), provided\n",
    "    they are admissible. The NetworkX library is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.initialize_graph()\n",
    "        self.directions = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"right\": (0, 1),\n",
    "            \"left\": (0, -1),\n",
    "        }\n",
    "\n",
    "    def initialize_graph(self):\n",
    "        # The graph is initialized and saved\n",
    "        \"\"\"\n",
    "        Initializes the graph. It loop over every corridor pixel over the rails_map matrix and adds\n",
    "        its corresponding node. For each new node, the presence of neighbors is checked, and they are\n",
    "        eventually added. Finally, those pixel who are not\n",
    "        \"\"\"\n",
    "\n",
    "        self.graph = nx.Graph()  # NetworkX-provided data structure to represent a graph\n",
    "        rails_map = np.load(\"saved_objects/rails_matrix.npy\").astype(int)\n",
    "        corridors_color = 1\n",
    "        m, n = np.shape(rails_map)\n",
    "\n",
    "        # Loop over all pixels\n",
    "        for row in range(m):\n",
    "            for col in range(n):\n",
    "                color = rails_map[row, col]\n",
    "                if (color == corridors_color) and (\n",
    "                    (row, col) not in self.graph.nodes\n",
    "                ):  # if corridor\n",
    "                    self.graph.add_node((row, col))  # add node to the graph\n",
    "\n",
    "                    # Loop over the neighbors and establish edge if necessary\n",
    "                    for offset in [(0, 1), (0, -1), (-1, 0), (1, 0)]:\n",
    "                        neighbor = (row + offset[0], col + offset[1])\n",
    "\n",
    "                        if (\n",
    "                            (0 <= neighbor[0] <= m - 1)\n",
    "                            and (0 <= neighbor[1] <= n - 1)\n",
    "                            and (rails_map[neighbor[0], neighbor[1]] == corridors_color)\n",
    "                        ):  # if not out of bounds and colored appropriately\n",
    "                            self.graph.add_edge((row, col), neighbor)\n",
    "\n",
    "    def nearest_node_to_pixel(self, pixel_coords):\n",
    "        \"\"\"\n",
    "        This function looks for the node which has the closest key to the given pixel (L1 distance)\n",
    "        and returns a tuple with its coordinates.\n",
    "        \"\"\"\n",
    "        nodes_arr = np.asarray(self.graph.nodes())\n",
    "        closest_node = nodes_arr[\n",
    "            np.argmin(np.linalg.norm(nodes_arr - pixel_coords, ord=1, axis=1))\n",
    "        ]\n",
    "        return tuple(closest_node)\n",
    "\n",
    "    def get_distance(self, source, target):\n",
    "        \"\"\"\n",
    "        Computes the shortest distance from source to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        \"\"\"\n",
    "        return nx.shortest_path_length(self.graph, source, target)\n",
    "\n",
    "    def get_distance_after_source_movement(self, source, movement, target):\n",
    "        \"\"\"\n",
    "        Computes the shortest distance from the source after a given movement to target.\n",
    "        Source is likely to be Pacman, while targets can be the ghosts for example.\n",
    "        \"\"\"\n",
    "        return nx.shortest_path_length(\n",
    "            self.graph, self.nextNode(source, movement), target\n",
    "        )\n",
    "\n",
    "    def nextNode(self, node, movement):\n",
    "        \"\"\"\n",
    "        Returns the node of the graph reached performing a step in a given direction from a given node.\n",
    "        \"\"\"\n",
    "        possibleMovements = self.getPossibleMovements(node)\n",
    "        if movement in possibleMovements:\n",
    "            return (\n",
    "                node[0] + self.directions[movement][0],\n",
    "                node[1] + self.directions[movement][1],\n",
    "            )\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    def getPossibleMovements(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of possible movements (up, down, right, left) that can be done starting\n",
    "        from a given node of the graph.\n",
    "        \"\"\"\n",
    "        neighbours = self.getNeighbours(node)\n",
    "        possibleMovements = []\n",
    "        for movement in self.directions.keys():\n",
    "            newNode = (\n",
    "                node[0] + self.directions[movement][0],\n",
    "                node[1] + self.directions[movement][1],\n",
    "            )\n",
    "            if newNode in neighbours:\n",
    "                possibleMovements.append(movement)\n",
    "        return possibleMovements\n",
    "\n",
    "    def getNeighbours(self, node):\n",
    "        \"\"\"\n",
    "        Returns the list of neighbors of a given node in the graph.\n",
    "        \"\"\"\n",
    "        neighbours = []\n",
    "        for neighbour in nx.all_neighbors(self.graph, node):\n",
    "            neighbours.append(neighbour)\n",
    "        return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.463705Z",
     "start_time": "2019-03-01T04:16:45.305848Z"
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTOR (gym frame -> clever representation)\n",
    "\n",
    "class Pacman_features_extractor():\n",
    "    def __init__(self, initial_screen):\n",
    "        self.positions = {\n",
    "            \"pacman\": None,\n",
    "            \"ghosts\": None,\n",
    "            \"foods\": None,\n",
    "            \"special_food\": None,\n",
    "        }\n",
    "        self.ghosts_scared = False\n",
    "        self.epsilon = 3\n",
    "        self.railwayGraph = RailwayGraph()\n",
    "\n",
    "        self.initialize_foods()\n",
    "        self.update(initial_screen)\n",
    "\n",
    "    def update(self, screen):\n",
    "        \"\"\"\n",
    "        Given a screen, it updates the position and state of all objects.\n",
    "        \"\"\"\n",
    "        self.update_guys(screen)\n",
    "        self.update_foods()\n",
    "\n",
    "    # List of features |->\n",
    "\n",
    "    def nearest_food_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"foods\")\n",
    "\n",
    "    def nearest_ghost_distance(self):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest ghost.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"ghosts\")\n",
    "\n",
    "    def nearest_special_food_distance():\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and the nearest special food.\n",
    "        \"\"\"\n",
    "        return self.nearest_entity_distance_from_pacman(\"special_food\")\n",
    "\n",
    "    def ghost_are_scared():\n",
    "        \"\"\"\n",
    "        Return true if ghosts are scared, false otherwise.\n",
    "        \"\"\"\n",
    "        return self.ghosts_scared\n",
    "    # <-| List of features (end)\n",
    "\n",
    "    \n",
    "    # Classes for features initialization/update/passing\n",
    "    \n",
    "    def update_guys(self, screen):\n",
    "        self.update_ghosts_scared(screen)\n",
    "        raw_guys_positions = self.extract_raw_guys_positions(screen)\n",
    "        self.positions[\"pacman\"] = self.railwayGraph.nearest_node_to_pixel(\n",
    "            raw_guys_positions[\"pacman\"]\n",
    "        )\n",
    "\n",
    "        # case 1: ghost are visible (scared or not)\n",
    "        if raw_guys_positions[\"ghosts\"] != []:  # I hope this is the right condition\n",
    "            # for pos in (raw_guys_positions['ghosts']):\n",
    "            self.positions[\"ghosts\"] = [\n",
    "                self.railwayGraph.nearest_node_to_pixel(pos)\n",
    "                for pos in raw_guys_positions[\"ghosts\"]\n",
    "            ]\n",
    "\n",
    "        # case 2: ghost are not visible -> just do nothing, we keep the old positions\n",
    "\n",
    "    def initialize_foods(self):\n",
    "        \"\"\"\n",
    "        It set the initial position of every food based on a-priori knowledge.\n",
    "        \"\"\"\n",
    "\n",
    "        foods_list = self.food_initial_raw_positions()\n",
    "        foods_nodes = []\n",
    "        for food_pos in foods_list:\n",
    "            foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(food_pos))\n",
    "        self.positions[\"foods\"] = foods_nodes\n",
    "\n",
    "        sp_foods_list = self.sp_food_initial_raw_positions()\n",
    "        sp_foods_nodes = []\n",
    "        for sp_food_pos in sp_foods_list:\n",
    "            sp_foods_nodes.append(self.railwayGraph.nearest_node_to_pixel(sp_food_pos))\n",
    "        self.positions[\"special_food\"] = sp_foods_nodes\n",
    "\n",
    "    def update_foods(self):\n",
    "        foods_distances = np.asarray(\n",
    "            [\n",
    "                np.linalg.norm(\n",
    "                    np.asarray(food_pos) - np.asarray(self.positions[\"pacman\"]), ord=1\n",
    "                )\n",
    "                for food_pos in self.positions[\"foods\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if np.min(foods_distances) < self.epsilon:\n",
    "            self.positions[\"foods\"].pop(np.argmin(foods_distances))\n",
    "\n",
    "        sp_foods_distances = np.asarray(\n",
    "            [\n",
    "                np.linalg.norm(\n",
    "                    np.asarray(sp_food_pos) - np.asarray(self.positions[\"pacman\"]),\n",
    "                    ord=1,\n",
    "                )\n",
    "                for sp_food_pos in self.positions[\"special_food\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if np.min(sp_foods_distances) < self.epsilon:\n",
    "            self.positions[\"special_food\"].pop(np.argmin(sp_foods_distances))\n",
    "\n",
    "    def nearest_entity_distance_from_pacman(self, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity.\n",
    "        \"\"\"\n",
    "\n",
    "        beast_pos = self.positions[\"pacman\"]\n",
    "        entity_positions = self.positions[entity_name]\n",
    "        return min(\n",
    "            [\n",
    "                self.railwayGraph.get_distance(beast_pos, e_pos)\n",
    "                for e_pos in entity_positions\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def nearest_entity_distance_from_pacman_after_movement(self, movement, entity_name):\n",
    "        \"\"\"\n",
    "        Returns the distance between the pacman beast and a given entity after a given movement of the beast.\n",
    "        \"\"\"\n",
    "\n",
    "        beast_pos = self.positions[\"pacman\"]\n",
    "        entity_positions = self.positions[entity_name]\n",
    "        return min(\n",
    "            [\n",
    "                self.railwayGraph.get_distance_after_source_movement(\n",
    "                    beast_pos, movement, e_pos\n",
    "                )\n",
    "                for e_pos in entity_positions\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def update_ghosts_scared(self, screen):\n",
    "        \"\"\"\n",
    "        Update ghosts_scared variable according to the given screen.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ghosts_scared = False\n",
    "\n",
    "    def extract_raw_guys_positions(self, screen):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with the raw positions of all 'guys', extracted from the given screen.\n",
    "        \"\"\"\n",
    "        guys_pos = self.PacmanAndGhostsCoords(screen)\n",
    "        return {\"pacman\": guys_pos[0], \"ghosts\": guys_pos[1]}\n",
    "\n",
    "    def food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/food_coords.npy\"))\n",
    "\n",
    "    def sp_food_initial_raw_positions(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the raw positions of all initial foods known a-priori.\n",
    "        \"\"\"\n",
    "        return list(np.load(\"saved_objects/special_food_coords.npy\"))\n",
    "\n",
    "    def center(self, SpecificMatrix):\n",
    "        \"\"\"\n",
    "        Given a matrix with 1 where lies the object you want to detect and 0 elsewhere,\n",
    "        the position of the center of the object is returned.\n",
    "        \"\"\"\n",
    "        a = np.where(SpecificMatrix == 1)\n",
    "        y = a[0]\n",
    "        x = a[1]\n",
    "\n",
    "        x_bar = (x.max() + x.min()) / 2\n",
    "        y_bar = (y.max() + y.min()) / 2\n",
    "\n",
    "        return (x_bar, y_bar)\n",
    "\n",
    "    def find_location(self, screen, value):\n",
    "        \"\"\"\n",
    "        Find the object corresponding to value within the matrix. If it is not present None is returned.\n",
    "        \"\"\"\n",
    "        SpecificMatrix = (screen == value).astype(int)\n",
    "        if SpecificMatrix.sum() == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return self.center(SpecificMatrix)\n",
    "\n",
    "    def PacmanAndGhostsCoords(\n",
    "        self,\n",
    "        screen,\n",
    "        PacmanValue=42,\n",
    "        WallsFoodValue=74,\n",
    "        GhostsValues=[70, 38, 184, 88],\n",
    "        ghosts_scared=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Given the matrix of the screen, a list with the positions of all the relevant objects is returned.\n",
    "        \"\"\"\n",
    "        pacman_coords = self.find_location(screen, PacmanValue)\n",
    "\n",
    "        if ghosts_scared:\n",
    "            pass\n",
    "        else:\n",
    "            ghosts_coords = []\n",
    "            for ghost_value in GhostsValues:\n",
    "                location = self.find_location(screen, ghost_value)\n",
    "                if location != None:\n",
    "                    ghosts_coords.append(self.find_location(screen, ghost_value))\n",
    "\n",
    "        return [pacman_coords, ghosts_coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.567655Z",
     "start_time": "2019-03-01T04:16:45.470991Z"
    }
   },
   "outputs": [],
   "source": [
    "# INTERFACE VERIFICATION\n",
    "\n",
    "class RL_Environment(ABC):\n",
    "    @abstractmethod\n",
    "    def getState(self):\n",
    "        \"\"\"\n",
    "        Should return the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Should return the list of all possible actions.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Should return the last reward received.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a and it's state changes.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def psi(self, s, a):\n",
    "        \"\"\"\n",
    "        Should return relevant features of the given state-action pair\n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "\n",
    "#class RL_system(ABC):\n",
    "#    @abstractmethod\n",
    "#    def psi(self, s, a):\n",
    "#        \"\"\"\n",
    "#        Should return relevant features of the given state-action pair\n",
    "#        as a dictionary of (feature name - feature value).\n",
    "#        \"\"\"\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.690348Z",
     "start_time": "2019-03-01T04:16:45.576397Z"
    }
   },
   "outputs": [],
   "source": [
    "# RL ENVIRONMENT (pt. 1)\n",
    "\n",
    "class pacman_RL_environment(RL_Environment):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"MsPacman-ram-v0\")\n",
    "\n",
    "        self.state = self.env.reset()  # env ram representation of the current state\n",
    "        self.skip_intro()  # the firsts steps you can't do anything, so it's better to skip them\n",
    "        self.current_reward = 0  # last reward received\n",
    "        self.cumulative_reward = 0\n",
    "        self.game_over = False\n",
    "\n",
    "        # the features_extractor is here because it has (and need) a state\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen())\n",
    "\n",
    "    def getState(self):\n",
    "        \"\"\"\n",
    "        Returns the current environment state as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "\n",
    "        # some examples of state features\n",
    "        features = {}\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            for movement in [\"up\", \"down\", \"right\", \"left\"]:\n",
    "                features[\n",
    "                    \"nearest_\" + entity + \"_distance_after_going_\" + movement\n",
    "                ] = self.features_extractor.nearest_entity_distance_from_pacman_after_movement(\n",
    "                    movement, entity\n",
    "                )\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            features[\n",
    "                \"nearest_\" + entity + \"_distance\"\n",
    "            ] = self.features_extractor.nearest_entity_distance_from_pacman(entity)\n",
    "\n",
    "        # features[\"ghost_are_scared\"] = features_extractor.ghost_are_scared()\n",
    "        # features[\"actual_time_step\"] =\n",
    "        # features[\"last_scared_ghost_time_step\"] =\n",
    "\n",
    "        return features\n",
    "\n",
    "    def getActions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of all possible actions as strings.\n",
    "        \"\"\"\n",
    "        return list(self.actions_dict().keys())\n",
    "    \n",
    "    def psi(self, s, a):\n",
    "        \"\"\"\n",
    "        Returns relevant features of the given state-action pair\n",
    "        as a dictionary of (feature name - feature value).\n",
    "        \"\"\"\n",
    "\n",
    "        # these are just examples taken from the paper\n",
    "        features = {}\n",
    "\n",
    "        # for entity in ['ghosts', 'foods', 'special_food']:\n",
    "        #    features[\"distance_of_the_closest_\" + entity] \\\n",
    "        #    = s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "\n",
    "        for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "            features[\"getting_closer_to\" + entity] = (\n",
    "                s[\"nearest_\" + entity + \"_distance\"]\n",
    "                - s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "            )\n",
    "\n",
    "        # features[\"distance_of_the_closest_food\"] = distance_of_the_next_closest_food(s,a)\n",
    "        # features[\"distance_of_the_closest_ghost\"] = distance_of_the_closest_ghost(s,a)\n",
    "        # features[\"food_will_be_eaten\"] = food_will_be_eaten(s,a)\n",
    "        # features[\"ghost_collision_is_possible\"] = ghost_collision_is_possible(s,a)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def getReward(self):\n",
    "        \"\"\"\n",
    "        Returns the last reward received.\n",
    "        \"\"\"\n",
    "        return self.current_reward\n",
    "\n",
    "    def perform_action(self, a):\n",
    "        \"\"\"\n",
    "        The environment perform the action a (given as a string) and it's state changes.\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_action = self.actions_dict()[\n",
    "            a\n",
    "        ]  # translate the action from string to number\n",
    "        self.state, self.current_reward, self.game_over, info = self.env.step(\n",
    "            encoded_action\n",
    "        )\n",
    "        self.cumulative_reward += self.current_reward\n",
    "\n",
    "        # then we have to update the features extractor,\n",
    "        # since features extraction doesn't depend only on the current screen\n",
    "        self.features_extractor.update(self.getCurrentScreen())\n",
    "\n",
    "    def restart(self):\n",
    "        \"\"\"\n",
    "        Set the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "\n",
    "        self.state = self.env.reset()\n",
    "        self.features_extractor = Pacman_features_extractor(self.getCurrentScreen())\n",
    "\n",
    "    def game_is_over(self):\n",
    "        \"\"\"\n",
    "        Returns true if the game is over.\n",
    "        \"\"\"\n",
    "        return self.game_over\n",
    "\n",
    "    def actions_dict(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of (action name - action encoded).\n",
    "        The encoding is needed to give the commands to the env.\n",
    "        \"\"\"\n",
    "\n",
    "        actions_d = {\"up\": 1, \"down\": 4, \"right\": 2, \"left\": 3}\n",
    "        return actions_d\n",
    "\n",
    "    def getCurrentScreen(self):\n",
    "        \"\"\"\n",
    "        Returns the current game screen.\n",
    "        \"\"\"\n",
    "        return self.env.env.ale.getScreen().reshape(210, 160)\n",
    "\n",
    "    def skip_intro(self):\n",
    "        intro_duration = 90\n",
    "        for i in range(intro_duration):\n",
    "            self.env.step(1)\n",
    "\n",
    "    def getCumulativeReward(self):\n",
    "        \"\"\"\n",
    "        Returns the actual cumulative reward.\n",
    "        \"\"\"\n",
    "        return self.cumulative_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:45.807932Z",
     "start_time": "2019-03-01T04:16:45.695221Z"
    }
   },
   "outputs": [],
   "source": [
    "# RL ENVIRONMENT (pt. 2)\n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "# In that case, mind to comment the other class declaration.\n",
    "\n",
    "#class pacman_RL_system(RL_system):\n",
    "\n",
    "class pacman_RL_system():\n",
    "    def __init__(self, _environment):\n",
    "        self.environment = _environment\n",
    "        self.old_state = self.environment.getState()\n",
    "        self.learning_vector = (\n",
    "            self.initial_learning_vector()\n",
    "        )  # the list of parameters to learn\n",
    "        self.eps_greedy = 0  # probability to play a random action\n",
    "        self.discount_factor = 0.9  # specifies how much long term reward is kept\n",
    "        self.learning_rate = 0.5\n",
    "        \n",
    "\n",
    "# Left here in case a migration of `psi` to `RL_system()` is needed or wanted, as naive migration is impossible.\n",
    "\n",
    "    #def psi(self, s, a):\n",
    "    #    \"\"\"\n",
    "    #    Returns relevant features of the given state-action pair\n",
    "    #    as a dictionary of (feature name - feature value).\n",
    "    #    \"\"\"\n",
    "    #\n",
    "    #    ## these are just examples taken from the paper\n",
    "    #    features = {}\n",
    "    #\n",
    "    #    ## for entity in ['ghosts', 'foods', 'special_food']:\n",
    "    #    ##    features[\"distance_of_the_closest_\" + entity] \\\n",
    "    #    ##    = s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "    #\n",
    "    #    for entity in [\"ghosts\", \"foods\", \"special_food\"]:\n",
    "    #        features[\"getting_closer_to\" + entity] = (\n",
    "    #            s[\"nearest_\" + entity + \"_distance\"]\n",
    "    #            - s[\"nearest_\" + entity + \"_distance_after_going_\" + a]\n",
    "    #        )\n",
    "    #\n",
    "    #    ## features[\"distance_of_the_closest_food\"] = distance_of_the_next_closest_food(s,a)\n",
    "    #    ## features[\"distance_of_the_closest_ghost\"] = distance_of_the_closest_ghost(s,a)\n",
    "    #    ## features[\"food_will_be_eaten\"] = food_will_be_eaten(s,a)\n",
    "    #    ## features[\"ghost_collision_is_possible\"] = ghost_collision_is_possible(s,a)\n",
    "    #\n",
    "    #    return features\n",
    "\n",
    "    def learn(self, iterations):\n",
    "        \"\"\"\n",
    "        Performs the learning steps a specified number of times.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(iterations):\n",
    "            if self.environment.game_is_over():\n",
    "                break\n",
    "            self.learning_step()\n",
    "\n",
    "        print(\"cumulative reward:\", self.environment.getCumulativeReward())\n",
    "\n",
    "        return self.learning_vector\n",
    "\n",
    "    def learning_step(self):\n",
    "        \"\"\"\n",
    "        This is the Q-learning routine.\n",
    "        \"\"\"\n",
    "\n",
    "        # s = self.environment.getState() # current state\n",
    "        s = (\n",
    "            self.old_state\n",
    "        )  # calculated during the previous update_learning_vector(s,a,r)\n",
    "        # print(s)\n",
    "        a = self.policy(s)  # action to perform according to the policy\n",
    "        print(\"action:\", a)\n",
    "        self.environment.perform_action(a)\n",
    "        r = self.environment.getReward()  # gained reward\n",
    "        print(\"reward:\", r)\n",
    "        print(\"psi:\", self.action_state_features_vector(s, a))\n",
    "        self.update_learning_vector(s, a, r)\n",
    "        print(\"vector:\", self.learning_vector)\n",
    "        print(\"-------------------:\")\n",
    "\n",
    "    def policy(self, s):\n",
    "        \"\"\"\n",
    "        Returns the action to perform in the state s according to a policy.\n",
    "        \"\"\"\n",
    "        return self.best_Q_policy(s)\n",
    "\n",
    "    def Q(self, s, a):\n",
    "        \"\"\"\n",
    "        This is the Q function, it returns the expected future discounted reward\n",
    "        for taking action a ∈ A in state s ∈ S.\n",
    "        \"\"\"\n",
    "        return self.learning_vector.dot(self.action_state_features_vector(s, a))\n",
    "\n",
    "    def update_learning_vector(self, s, a, r):\n",
    "        \"\"\"\n",
    "        This function updates the learning vector.\n",
    "            a is the last performed action,\n",
    "            s is the previous state,\n",
    "            r is the reward that has been generated by performing a in state s. \"\"\"\n",
    "\n",
    "        currentState = self.environment.getState()\n",
    "        max_Q = max(\n",
    "            [self.Q(currentState, action) for action in self.environment.getActions()]\n",
    "        )\n",
    "        difference = r + self.discount_factor * max_Q - self.Q(s, a)\n",
    "        self.learning_vector += (\n",
    "            self.learning_rate * difference * self.action_state_features_vector(s, a)\n",
    "        )\n",
    "        self.old_state = currentState\n",
    "\n",
    "    def best_Q_policy(self, s):\n",
    "        \"\"\"\n",
    "        For a given state It returns the action that maximize the Q_function, but it\n",
    "        can also return a random action with probability = eps_greedy.\n",
    "        \"\"\"\n",
    "\n",
    "        actions = self.environment.getActions()\n",
    "\n",
    "        if self.random_boolean(self.eps_greedy):\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "        i = np.argmax([self.Q(s, a) for a in actions])\n",
    "        return actions[i]\n",
    "\n",
    "    def random_boolean(self, probability_of_true):\n",
    "        \"\"\"\n",
    "        It returns true with the given probability, false otherwise.\n",
    "        \"\"\"\n",
    "        return np.random.random_sample() < probability_of_true\n",
    "\n",
    "    def initial_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the initial configuration of the learning vector.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.zeros(\n",
    "            len(\n",
    "                self.action_state_features_vector(\n",
    "                    self.old_state, self.environment.getActions()[0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def current_state_vector(self):\n",
    "        \"\"\"\n",
    "        It returns the vector of numerical values representing the current state.\n",
    "        It basically extract the values from the state dictionary of the environment.\n",
    "        \"\"\"\n",
    "        return np.asarray(list(self.environment.getState().values()))\n",
    "\n",
    "    def action_state_features_vector(self, s, a):\n",
    "        \"\"\"\n",
    "        It returns a vector of numerical values representing relevant features of the state-action pair.\n",
    "        It basically extract the values from the psi(s,a) dictionary of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.asarray(list(self.environment.psi(s, a).values()))\n",
    "\n",
    "    def reset_environment(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial configuration.\n",
    "        \"\"\"\n",
    "        self.environment.restart()\n",
    "\n",
    "    def reset_learning_vector(self):\n",
    "        \"\"\"\n",
    "        It sets the environment to the initial predefined value.\n",
    "        \"\"\"\n",
    "        self.learning_vector = self.initial_learning_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:16:46.796501Z",
     "start_time": "2019-03-01T04:16:45.812901Z"
    }
   },
   "outputs": [],
   "source": [
    "# INITIALIZE AGENT\n",
    "\n",
    "agent = pacman_RL_system(pacman_RL_environment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T04:18:17.705144Z",
     "start_time": "2019-03-01T04:16:46.797855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 0 0]\n",
      "vector: [0. 0. 0.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [-1 -1  1]\n",
      "vector: [-5. -5.  5.]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [-5.25 -4.75  4.75]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.9875 -5.0125  4.4875]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.764375 -5.235625  4.264375]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.57471875 -5.42528125  4.07471875]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-10.62901719   0.62901719  10.12901719]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.06935258 10.1886818  19.6886818 ]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.44040055  7.67892867 17.17892867]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.17087284  5.06765528 14.56765528]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 4.43032608 -0.53354364  8.96645636]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1 -1  1]\n",
      "vector: [ 0.6965163  -4.26735342 12.70026613]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [-4.02730857  0.45647145  7.97644127]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.62301106  3.86076895 11.38073877]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.73092483  2.50683306 10.02680287]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [0.06769679 1.84360502 9.36357483]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.49604704  1.27986119  8.799831  ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.97522929  0.80067893  8.32064874]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.38253421  0.39337401  7.91334382]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.72874339  0.04716483  7.56713464]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.46715214  1.30875608  8.82872589]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [0.48351649 0.35808744 7.87805726]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 0.04753343 -0.07789561  7.4420742 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.32305217 -0.44848122  7.0714886 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.63804993 -0.76347898  6.75649084]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.90579803 -1.03122707  6.48874274]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.13338391 -1.25881296  6.26115686]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.32683191 -1.45226095  6.06770886]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.49126271 -1.61669175  5.90327806]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.63102889 -1.75645793  5.76351188]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.74983014 -1.87525919  5.64471063]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.85081121 -1.97624025  5.54372956]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.93664511 -2.06207416  5.45789566]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-2.00960393 -2.13503298  5.38493684]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-2.07161893 -2.19704797  5.32292184]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.25987464 -0.38530369  7.13466613]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.09058721 -0.73576554  6.78420427]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.38847979 -1.03365812  6.4863117 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.64168848 -1.28686681  6.23310301]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.85691587 -1.50209419  6.01787562]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.03985914 -1.68503747  5.83493234]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.19536093 -1.84053926  5.67943056]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.32753745 -1.97271578  5.54725404]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.43988749 -2.08506582  5.434904  ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.53538502 -2.18056335  5.33940646]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.61655793 -2.26173625  5.25823356]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.6855549  -2.33073322  5.18923659]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.74420232 -2.38938065  5.13058917]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.79405263 -2.43923096  5.08073886]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.83642539 -2.48160372  5.03836609]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.87244224 -2.51762057  5.00234924]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.90305657 -2.54823489  4.97173492]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.92907874 -2.57425706  4.94571275]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.95119759 -2.59637591  4.9235939 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.96999861 -2.61517693  4.90479288]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.98597947 -2.6311578   4.88881201]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.99956321 -2.64474154  4.87522828]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.01110939 -2.65628771  4.8636821 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.02092364 -2.66610196  4.85386785]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.02926575 -2.67444408  4.84552574]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.03635655 -2.68153487  4.83843494]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.04238372 -2.68756205  4.83240777]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.04750682 -2.69268515  4.82728467]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.05186146 -2.69703978  4.82293003]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.0555629  -2.70074122  4.81922859]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.05870912 -2.70388745  4.81608237]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06138341 -2.70656174  4.81340808]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06365656 -2.70883488  4.81113493]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06558873 -2.71076706  4.80920276]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06723108 -2.71240941  4.80756041]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06862708 -2.7138054   4.80616441]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.06981367 -2.714992    4.80497782]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.07082228 -2.71600061  4.80396921]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.0716796  -2.71685792  4.80311189]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: [-2.92759169  2.28241336  9.80238317]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.17697227  1.53179395  9.05176376]\n",
      "-------------------:\n",
      "action: down\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.53894578  0.89376745  8.41373726]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.99662325  0.35144492  7.87141474]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-5.53564911  4.89047078 12.41044059]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.39382108  3.74864276 11.26861257]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.42326726  2.77808893 10.29805875]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.59829651  1.95311819  9.473088  ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.89707138  1.25189305  8.77186287]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-6.30103001  5.65585169 13.1758215 ]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.62653216 -1.27171048  6.24825933]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 0.34637811 -1.55186453  5.96810528]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 0.10824717 -1.78999548  5.72997434]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 1.41940935 -0.4788333   7.04113652]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 10.0\n",
      "psi: [-1 -1  1]\n",
      "vector: [-4.12208107 -6.02032372 12.58262693]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-4.24409218 -6.14233482 12.46061583]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-4.34780162 -6.24604427 12.35690639]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-4.43595465 -6.33419729 12.26875336]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-4.51088472 -6.40912736 12.19382329]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 6.19363935  4.2953967  22.89834735]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [ 4.95380985  5.5352262  21.65851785]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [ 3.89995477  6.58908128 20.60466278]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 0.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [ 3.00417796  7.48485809 19.70888597]\n",
      "-------------------:\n",
      "action: right\n",
      "reward: 10.0\n",
      "psi: [ 1 -1  1]\n",
      "vector: [11.27537979 -0.78634374 27.98008779]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [12.071298   -1.58226195 27.18416958]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.88366028  8.60537577 37.3718073 ]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.50938189  6.2123336  34.97876513]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-2.54346773  4.17824776 32.94467929]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 0.72755931  7.44927479 36.21570632]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.49206772  5.22964777 33.9960793 ]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-3.37875068  3.3429648  32.10939634]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-4.95022391  1.77149157 30.5379231 ]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1 -1  1]\n",
      "vector: [-3.26439114  3.45732434 28.85209033]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1 -1  1]\n",
      "vector: [-7.00507317 -0.28335768 32.59277236]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-8.27029024 -1.54857476 31.32755528]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-9.34572476 -2.62400927 30.25212077]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.84869181  4.87302367 37.74915371]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.96132572  5.98565758 38.86178762]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-2.39043855  6.55654476 39.4326748 ]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [2.85443587e-02 4.13756185e+00 3.70136919e+01]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.91532017  7.08142638 39.95755642]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 2.20618313  1.95992308 34.83605312]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.72948965 -1.97574971 30.90038033]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.35975705 -5.06499641 27.81113363]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 5.15446234 -1.27029112 31.60583892]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 3.37996183 -3.04479163 29.83133841]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 1.8716364  -4.55311706 28.32301298]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.09491298 -7.51966643 25.35646361]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.14832747 -8.46625194 24.4098781 ]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.65627021 -9.27084962 23.60528042]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.65982176 -4.95475765 27.92137239]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.32849993 -6.28607948 26.59005056]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.19687638 -7.41770303 25.45842701]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.23499636 -8.37958304 24.49654699]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.58260165 -9.19718106 23.67894898]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.27755997 -9.89213937 22.98399067]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.13172547 -5.48285394 27.3932761 ]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.87961809 -6.73496132 26.14116872]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.81532681 -7.7992526  25.07687744]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.08932077 -8.70390018 24.17222986]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.85827122 -9.47285062 23.40327941]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.48812091 -5.1264585  27.74967154]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.18255421 -6.4320252  26.44410484]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.07282252 -7.54175689 25.33437315]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-5.12955058 -3.48502883 29.39110121]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.57776943 -5.03680998 27.83932006]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.25875545 -6.35582395 26.52030608]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.13759358 -7.47698583 25.39914421]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.18460598 -8.42997343 24.44615661]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.37456652 -4.24001289 28.63611715]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.93603298 -5.67854643 27.19758361]\n",
      "-------------------:\n",
      "action: left\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.71327947 -6.90129994 25.9748301 ]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-0.67393899 -7.94064042 24.93548962]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.20950042 -8.82407983 24.05205021]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.03957608 -4.57500333 28.30112671]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-2.65129111 -5.9632883  26.91284174]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.47124888 -7.14333053 25.73279951]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector: [-0.46821299 -8.14636642 24.72976362]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-4.61563248 -3.99894693 28.87718311]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-3.14093905 -5.47364036 27.40248968]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [-1.88744963 -6.72712978 26.14900026]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 0.87672104 -9.49130045 23.38482959]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 10.0\n",
      "psi: [1 1 1]\n",
      "vector: [ 4.3491596  -6.0188619  26.85726814]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 5.17362193 -6.84332423 26.03280581]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 5.87441491 -7.54411721 25.33201283]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 6.47008895 -8.13979125 24.73633879]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 6.97641188 -8.64611418 24.23001586]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 7.40678637 -9.07648867 23.79964137]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 7.77260468 -9.44230699 23.43382305]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.08820604 -2.75790834 30.1182217 ]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-1.31360537 -5.15971974 27.71641029]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ 1.06215426 -7.53547937 25.34065067]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [-0.83715085 -9.43478448 23.44134556]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ -0.09496526 -10.17697008  22.69915996]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [  0.5358925  -10.80782783  22.0683022 ]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [  1.07212159 -11.34405693  21.53207311]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [  0.56300689 -10.83494222  22.04118782]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -0.02545574 -11.42340485  21.45272519]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -0.52564897 -11.92359808  20.95253196]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -0.95081321 -12.34876232  20.52736772]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -1.31220282 -12.71015193  20.16597811]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -1.61938399 -13.0173331   19.85879694]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -1.88048798 -13.27843709  19.59769295]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.10242638 -13.50037549  19.37575455]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.29107401 -13.68902312  19.18710692]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.4514245  -13.84937361  19.02675643]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.58772242 -13.98567153  18.89045851]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.70357564 -14.10152475  18.77460528]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.80205089 -14.2         18.67613004]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.88575485 -14.28370396  18.59242608]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -2.95690321 -14.35485232  18.52127772]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.01737932 -14.41532843  18.46080161]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.06878401 -14.46673312  18.40939692]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.112478   -14.51042711  18.36570293]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.14961789 -14.547567    18.32856304]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.1811868  -14.57913591  18.29699413]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.20802037 -14.60596948  18.27016056]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.23082891 -14.62877802  18.24735202]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -3.25021616 -14.64816527  18.22796477]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [1 1 1]\n",
      "vector: [ -1.80409806 -13.20204717  19.67408287]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [0 1 1]\n",
      "vector: [ -1.80409806 -12.71380483  20.16232521]\n",
      "-------------------:\n",
      "action: up\n",
      "reward: 0.0\n",
      "psi: [-1  1  1]\n",
      "vector: [ -1.34146713 -13.17643575  19.69969429]\n",
      "-------------------:\n",
      "cumulative reward: 220.0\n",
      "CPU times: user 1min 29s, sys: 350 ms, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "# DO STUFF\n",
    "\n",
    "iterations: int = 200\n",
    "    \n",
    "try:\n",
    "    get_ipython().run_line_magic(\"time\", \"agent.learn(iterations)\")\n",
    "except NameError:\n",
    "    agent.learn(iterations)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
